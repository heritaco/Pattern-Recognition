{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Minimizing the classification error probability**"
      ],
      "metadata": {
        "id": "pZbBHMkl1OcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $R_1$ be the region of the feature space in which we decide in favor of\n",
        "$\\omega_1$ and $R_2$ be the corresponding region for $\\omega_2$.\n",
        "\n",
        "Then an **error** is made if $x \\in R_1$, although it belongs to $\\omega_2$ or if $x \\in R_2$, although it belongs to $\\omega_1$. That is,\n",
        "$$P_e = P(x\\in R_2, \\omega_1) + P(x\\in R_1, \\omega_2)$$\n",
        "Where $P(\\cdot,\\cdot)$ is the **joint probability** of two events.\n"
      ],
      "metadata": {
        "id": "AnWr1XQ91RnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By definition, the **conditional probability** of $B$ assuming $A$ is\n",
        "$$P(B|A) = \\frac{P(B,A)}{P(A)}$$"
      ],
      "metadata": {
        "id": "8R72UDt82aR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, the total error becomes\n",
        "\\begin{eqnarray}\n",
        "P_e &=& P(x\\in R_2 | \\omega_1) P(\\omega_1)+P(x\\in R_1 | \\omega_2) P(\\omega_2)\\\\\n",
        "&=& P(\\omega_1) \\int_{R_2} p(x|\\omega_1)dx + P(\\omega_2) \\int_{R_1} p(x|\\omega_2)dx\\tag{eq1}\n",
        "\\end{eqnarray}\n",
        "Where $ p(x|\\omega_1)$ is the **conditional probability density function** of $x$ given $\\omega_1$, which describes how the values of $x$ are distributed within the class $\\omega_1$.\n"
      ],
      "metadata": {
        "id": "VDDzNl4m2pVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bayes' rule** states that\n",
        "$$P(\\omega_1 | x) = \\frac{p(x|\\omega_1)P(\\omega_1)}{p(x)}$$"
      ],
      "metadata": {
        "id": "rpK6M1fc4Bv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Factoring we obtain\n",
        "$$\\frac{P(\\omega_1|x)p(x)}{P(\\omega_1)}= p(x|\\omega_1)$$"
      ],
      "metadata": {
        "id": "IiYNcMVH4akt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Substituting in **(eq1)**, we have that\n",
        "$$P_e = \\int_{R_2} P(\\omega_1|x)p(x)dx+\\int_{R_1} P(\\omega_2|x)p(x)dx \\tag{eq2}$$"
      ],
      "metadata": {
        "id": "-W_Yb5Yn4bQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the **law of total probability**\n",
        "$$P(\\omega_1) = \\int_{\\Omega} P(\\omega_1 | x) p(x) dx$$"
      ],
      "metadata": {
        "id": "wmeXvOfD86MV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the uniont of the regions $R_1$, $R_2$ partition the space, that is\n",
        "$\\Omega = R_1 \\cup R_2$ and $R_1 \\cap R_2=\\emptyset$, we have that\n",
        "\\begin{eqnarray} P(\\omega_1) &=& \\int_{R_1 \\cup R_2} P(\\omega_1|x) p(x)dx \\\\\n",
        " &=& \\int_{R_1}  P(\\omega_1|x) p(x)dx + \\int_{R_2} P(\\omega_1|x) p(x)dx\\end{eqnarray}\n",
        "Then,\n",
        "$$\\int_{R_2} P(\\omega_1|x)p(x)dx= P(\\omega_1)-\\int_{R_1}P(\\omega_1|x)p(x)dx$$"
      ],
      "metadata": {
        "id": "X5OBhgx4IgyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Substituting in **(eq2)**, we obtain\n",
        "\\begin{eqnarray} P_e &=& \\left( P(\\omega_1)-\\int_{R_1}P(\\omega_1|x)p(x)dx \\right)+\\int_{R_1} P(\\omega_2|x)p(x)dx\\\\\n",
        "&=& P(\\omega_1)+\\int_{R_1} \\left(-P(\\omega_1|x) +P(\\omega_2 | x) \\right)p(x) dx\\\\\n",
        "&=&P(\\omega_1)-\\int_{R_1} \\left(P(\\omega_1|x) -P(\\omega_2 | x) \\right)p(x) dx\n",
        "\\end{eqnarray}"
      ],
      "metadata": {
        "id": "CJ_O9jkxJzLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the first element is fixed, $P(\\omega_1)$, the **error is minimized** when the value of the integral is larger. This occurs when $R_1$ is the region of space in which\n",
        "$$P(\\omega_1 |x) > P(\\omega_2 |x)$$"
      ],
      "metadata": {
        "id": "wi3LCP4cJ3mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, $R_2$ becomes the region where the reverse is true. In other words, when\n",
        "$$P(\\omega_2 |x ) > P(\\omega_1|x)$$"
      ],
      "metadata": {
        "id": "GUAeMkE4LlQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, the **error is minimized** if the partitioning regions $R_1$ and $R_2$ of the feature space are chosen so that\n",
        "$$R_1: \\qquad P(\\omega_1|x) > P(\\omega_2|x)$$\n",
        "$$R_2: \\qquad P(\\omega_2 |x) > P(\\omega_1|x)$$"
      ],
      "metadata": {
        "id": "hCLfgcFILv_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **$M$-classes**"
      ],
      "metadata": {
        "id": "EWj3f4SVNez_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have dealt with the simple case of two classes. Generalizations to\n",
        "the multiclass case are straightforward. In a classification task with $M$ classes, $\\omega1, \\omega_2,\\ldots, \\omega_M$, an unknown pattern, represented by the feature vector $x$, is assigned to class $\\omega_i$ if\n",
        "$$P(\\omega_1 | x) > P(\\omega_j |x), \\qquad \\forall j\\neq i$$"
      ],
      "metadata": {
        "id": "Xcthneb_NhOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be shown that such a choice also **minimizes the probability of classification error**."
      ],
      "metadata": {
        "id": "MKBgTPsiNzXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discriminant functions**"
      ],
      "metadata": {
        "id": "z16e1nmbOB45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is by now clear that minimizing either the error probability  is equivalent to **partitioning** the feature space into $M$ regions, for a task with $M$ classes."
      ],
      "metadata": {
        "id": "ZWxo4qwkOHCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If regions $R_i$, $R_j$ happen to be **contiguous**, then they are separated by a **decision surface** in the multidimensional feature space. For the minimum error probability case, this is described by the equation\n",
        "$$P(\\omega_i | x) - P(\\omega_j|x) = 0$$"
      ],
      "metadata": {
        "id": "wZiqHiPyON38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the one side of the surface this difference is positive, and from the other\n",
        "it is negative.\n",
        "\n",
        "Sometimes, instead of working directly with probabilities, it may be more convenient, from a mathematical point of view, to work with an equivalent function of them, for example,\n",
        "$$g_i(x) \\equiv f (P(\\omega_i |x))$$\n",
        "where $f(\\cdot)$ is a **monotonically increasing function**. $g_i(x)$ is known as a **discriminant function**."
      ],
      "metadata": {
        "id": "JUuWwqjSOaqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **decision test**\n",
        "$$P(\\omega_i|x) > P(\\omega_j|x) \\qquad \\forall j\\neq i$$\n",
        "is now stated as\n",
        "$$\\text{classify} \\ x \\ \\text{in} \\ \\omega_i \\quad \\text{if} \\quad g_i(x) > g_j(x) \\quad j\\neq i$$"
      ],
      "metadata": {
        "id": "jfuGZzbJOuQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **decision surfaces**, separating contiguous regions, are described by\n",
        "$$g_{ij}(x) \\equiv g_i(x) - g_j(x) = 0, \\qquad i,j=1,2,\\ldots,M, \\ i\\neq j$$"
      ],
      "metadata": {
        "id": "ylTTqEaxO5Cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have approached the classification problem via **Bayesian probabilistic approache** and the goal was to **minimize the classification error probability** or the **risk**.\n",
        "\n",
        "However, as we will soon see, not all problems are well suited to such approaches.\n",
        "> For example, in many cases the involved pdfs are complicated and their estimation is not an easy task.\n",
        "\n",
        ">In such cases, it may be preferable to compute decision surfaces directly by means of alternative costs.\n",
        "\n",
        "Such approaches give rise to discriminant functions and decision surfaces, which\n",
        "are entities with no (necessary) relation to Bayesian classification, and they are, in general, **suboptimal** with respect to **Bayesian classifiers**."
      ],
      "metadata": {
        "id": "30N0v0yNPhAy"
      }
    }
  ]
}