{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL COMPARISON**"
      ],
      "metadata": {
        "id": "fh5kilnPEcUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**"
      ],
      "metadata": {
        "id": "ryPNrqjxXMzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose a classification problem with $N$ patterns and $M$ classes, where the patterns are labeled with the variable $i=1,\\ldots,N$ and the classes with $l=1,...,M$."
      ],
      "metadata": {
        "id": "mBAl0MJSEe1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classification, whether true or the result of a classification model, can be represented in different ways:"
      ],
      "metadata": {
        "id": "8qtxNHhiLjiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Set of sets representation**: As a set of sets, a set of classes consisting of the indices of the patterns within the class:\n",
        "$$y = \\left\\{\\begin{matrix} y_1 = \\{i \\ \\mid \\ x_i\\in C_1, \\ i=1,\\ldots, N\\}\\\\\n",
        "y_2 = \\{i \\ \\mid\\ x_i\\in C_2, \\ i=1,\\ldots, N\\}\\\\\n",
        "\\vdots\\\\\n",
        "y_M = \\{i \\ \\mid\\ x_i\\in C_M, \\ i=1,\\ldots, N\\}\\\\\n",
        "\\end{matrix}\\right\\}$$"
      ],
      "metadata": {
        "id": "KTey1fPNLkW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Vector representation**: As a vector of length $n$ where each element $i$ contains the label class $l$ to which it belongs.\n",
        "$$ y =[y_1, \\  y_2, \\  \\ldots, \\  y_N ], \\qquad \\text{where} \\ \\  y_i = l \\ | \\ x_i \\in C_l $$"
      ],
      "metadata": {
        "id": "XfgPksJhM7eH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Matrix representation** As a binary matrix of $M \\times N$, where each cell $i,l$ is equal to 1 if $x_i \\in C_l$, and 0 otherwise.\n",
        "$$ y = [ y_{l, i} ], \\qquad y_{l,i} = \\begin{cases} 1 & \\text{if}  \\quad x_i\\in C_l\\\\\n",
        "0 & \\text{if} \\quad x_i \\not\\in C_l \\end{cases}, \\qquad l=1,\\ldots, M, \\ i=1,\\ldots, N$$"
      ],
      "metadata": {
        "id": "GrV5qFZcOVOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, consider the following mappings:\n",
        "\n",
        "$$P(A, B) := \\frac{|A\\cap B|}{|B|}$$\n",
        "\n",
        "$$R(A, B) := \\frac{|A\\cap B|}{|A|}$$\n",
        "\n",
        "where $A$ and $B$ are sets."
      ],
      "metadata": {
        "id": "Z-TW16ONHk8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $y= \\{y_1, y_2, \\ldots, y_M \\}$ be the true classification and $\\hat{y} = \\{\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_M \\}$ the estimated classification  by a model (set of sets representation).\n",
        "\n"
      ],
      "metadata": {
        "id": "eZrf_homQJTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Global precision and recall**"
      ],
      "metadata": {
        "id": "CGMlxXxSXO34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can define the following local and global measures:\n"
      ],
      "metadata": {
        "id": "p0WhNdfuVeWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Precision**"
      ],
      "metadata": {
        "id": "zcQED7KnViER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Precision**  of class $C_l$:\n",
        "$$P_l :=  P( y_l, \\hat{y_l}) = \\frac{|y_l\\cap \\hat{y}_l|}{|\\hat{y}_l|}$$\n",
        "```\n",
        "metrics.precision_score(y_true, y_pred, average=None)\n",
        "```\n",
        "\n",
        "\n",
        "> **Precision-Macro** (global):\n",
        "$$ \\frac{1}{|M|} \\sum_{l=1}^M P(y_l,\\hat{y}_l)$$\n",
        "```\n",
        "metrics.precision_score(y_true, y_pred, average=\"macro\")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "> **Precision-Weighted** (global):\n",
        "$$ \\frac{1}{N}\\sum_{l=1}^M |y_l|P(y_l, \\hat{y}_l) $$\n",
        "```\n",
        "metrics.precision_score(y_true, y_pred, average=\"weighted\")\n",
        "```"
      ],
      "metadata": {
        "id": "ock26rfVVjj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recall**"
      ],
      "metadata": {
        "id": "k2R3lkz7W6tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Recall**  of class $C_l$:\n",
        "$$R_l :=  R( y_l, \\hat{y_l}) = \\frac{|y_l\\cap \\hat{y}_l|}{|y_l|}$$\n",
        "```\n",
        "metrics.recall_score(y_true, y_pred, average=None)\n",
        "```\n",
        "\n",
        "\n",
        "> **Recall-Macro** (global):\n",
        "$$ \\frac{1}{|M|} \\sum_{l=1}^M R(y_l,\\hat{y}_l)$$\n",
        "```\n",
        "metrics.recall_score(y_true, y_pred, average=\"macro\")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "> **Recall-Weighted** (global):\n",
        "$$ \\frac{1}{N}\\sum_{l=1}^M |y_l|R(y_l, \\hat{y}_l) $$\n",
        "```\n",
        "metrics.recall_score(y_true, y_pred, average=\"weighted\")\n",
        "```"
      ],
      "metadata": {
        "id": "lQPcS_btW9c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Jaccard similarity coefficient**\n",
        "\n"
      ],
      "metadata": {
        "id": "8LIoGXQE4ruO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jaccard similarity coefficient**, or simply **Jaccard measure**, measures the similarity of label sets $y_l$ and $\\hat{y}_l$ (in set of sets representation).\n",
        "\n",
        "The coefficient for each label $l$ is\n",
        "$$J_l = J(y_l, \\hat{y}_l) = \\frac{|y_l\\cap \\hat{y}_l|}{|y_l\\cup \\hat{y}_l|}$$\n",
        "\n",
        "For instance, if the label vectors are:\n",
        "$$ y = (0, 0, 1, 1, 2, 2, 2, 2) $$\n",
        "$$ \\hat{y} = (0, 1, 1, 0, 2, 2, 1, 2)$$\n",
        "\n",
        "Then\n",
        "$$ J_0 = J(y_0, \\hat{y}_0) = \\frac{1}{3}$$\n",
        "\n",
        "Since the first object is classified correctly, but there are 3 positions where label $0$ appears in one of the two vectors.\n",
        "\n",
        "```\n",
        "metrics.jaccard_score(y_true, y_pred, average=None)\n",
        "```"
      ],
      "metadata": {
        "id": "6ufdRugtXpQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Jaccard coefficient-Macro** (global):\n",
        "$$\\frac{1}{M} \\sum_{l=1}^M J(y_l, \\hat{y}_l)$$\n",
        "```\n",
        "metrics.jaccard_score(y_true, y_pred, average=\"macro\")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "> **Jaccard coefficient-Weighted** (global):\n",
        "$$\\frac{1}{N}\\sum_{l=1}^M |y_l| J(y_l, \\hat{y}_l)$$\n",
        "```\n",
        "metrics.jaccard_score(y_true, y_pred, average=\"weighted\")\n",
        "```"
      ],
      "metadata": {
        "id": "peC6OEsgliry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hamming loss function**"
      ],
      "metadata": {
        "id": "i8rRwNtq4oba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In information theory, the **Hamming distance** is defined as the number of positions at which two strings or vectors of the same length differ. Essentially, it quantifies the minimum number of substitutions needed to transform one string into the other."
      ],
      "metadata": {
        "id": "WTXVdWcZ5S4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ L_{hamming}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N  1(y_i \\neq \\hat{y}_i)$$"
      ],
      "metadata": {
        "id": "Y-Cnxe2y5eks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where $y_i$ and $\\hat{y}_i$ are label vectors (vector representation)"
      ],
      "metadata": {
        "id": "YxPrfwYo5lRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, if the label vectors are:\n",
        "$$ y = (0, 0, 1, 1, 2, 2, 2, 2) $$\n",
        "$$ \\hat{y} = (0, 1, 1, 0, 2, 2, 1, 2)$$\n",
        "\n",
        "Then\n",
        "$$ L_{hamming}(y, \\hat{y}) = \\frac{3}{8}$$\n",
        "\n",
        "Because the vectors differ in positions 2, 4 and 7."
      ],
      "metadata": {
        "id": "q2PbxaXh59ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "metrics.hamming_loss(y_true, y_pred)\n",
        "```"
      ],
      "metadata": {
        "id": "i6TIepla6rTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entropy and mutual information**"
      ],
      "metadata": {
        "id": "3aYMxXu1_RHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to compare classifications is by using the concepts of **information theory**, particularly, entropy and mutual information."
      ],
      "metadata": {
        "id": "TTRuN5ah_cxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by considering a discrete random variable $X$ and we ask how much\n",
        "information is received when we observe a specific value for this variable. The\n",
        "**amount of information** can be viewed as the *degree of surprise* on learning the value of $X$.\n",
        "\n",
        "> If we are told that a highly improbable event has just occurred, we will\n",
        "have received more information than if we were told that some very likely event\n",
        "has just occurred, and if we knew that the event was certain to happen we would\n",
        "receive no information.\n",
        "\n"
      ],
      "metadata": {
        "id": "GO67WXlY_7zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our **measure of information content** will therefore depend on the probability distribution $p(x)$, instance, the probability to belong to a certain class; and we therefore look for a quantity $h(x)$ that is a monotonic function of the probability $p(x)$ and that expresses the information content.\n",
        "\n",
        "> The form of $h(\\cdot)$ can be found by noting that if we have two events $x$ and $y$ that are unrelated, then the information gain from observing both of them should be the sum of the information gained from each of them separately, so that $$h(x, y) = h(x) + h(y)$$\n",
        "\n",
        "> Two unrelated events will be statistically independent and so $$p(x, y) = p(x)p(y)$$\n",
        "\n",
        "> From these two relationships, it is easily shown that $h(x)$ must be given by the logarithm of $p(x)$ and so we have\n",
        "$$ h(x) = -\\log_2 p(x)$$\n",
        "where the negative sign ensures that information is positive or zero.\n",
        "\n",
        "> Note that low probability of events $x$ corresponds to high information content.\n",
        "\n",
        "> The choice of basis for the logarithm is arbitrary, and for the moment we shall adopt the convention prevalent in information theory of using logarithms to the base of 2. In this case, as we shall see shortly, the units of $h(x)$ are **bits** (*binary digits*)."
      ],
      "metadata": {
        "id": "i_uu8BOhAldn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Entropy**"
      ],
      "metadata": {
        "id": "6n4uddy-H6nP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now suppose that a sender wishes to transmit the value of a random variable to\n",
        "a receiver. The **average amount of information** that they transmit in the process is obtained by taking the expectation of $h(x)$ with respect to the distribution $p(x)$ and is given by\n",
        "$$H(x) = -\\sum_x p(x) \\log_2 p(x)$$"
      ],
      "metadata": {
        "id": "gLUt1eUuCd0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This important quantity is called the **entropy** of the random variable $x$.\n",
        "\n",
        "> Note that $\\lim_{p\\rightarrow 0} p\\log_2 p=0$ and so we shall take $p(x) log_2 p(x) = 0$ whenever we encounter a value for $x$ such that $p(x) = 0$.\n",
        "\n",
        "> Entropy is always non-negative. It takes value 0 only when there is no uncertainty, namely when there is only one cluster.\n"
      ],
      "metadata": {
        "id": "0zbg4iyhCqki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example:\n"
      ],
      "metadata": {
        "id": "MaVKqcfKDnAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">> Consider a random variable $x$ having 8 possible states, each of which is equally likely. In order to communicate the value of $x$ to a receiver, we would need to transmit a message of length 3 bits. Notice that the entropy of this variable is given by\n",
        "$$ H(x) = -\\sum_{i=1}^8 \\frac{1}{8}\\log_2\\left(\\frac{1}{8}\\right)=-8\\frac{1}{8}\\log_2\\left(\\frac{1}{8}\\right) = 3 \\ \\text{bits}$$"
      ],
      "metadata": {
        "id": "kQVZc-kjDUMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> Now consider an example of a variable having 8 possible states for which the respective probabilities are given by\n",
        "$$\\left(\\frac{1}{2}, \\frac{1}{4},\\frac{1}{8}, \\frac{1}{16}, \\frac{1}{64}, \\frac{1}{64}, \\frac{1}{64}, \\frac{1}{64}\\right)$$\n",
        "The entropy in this case is given by\n",
        "$$H(x) = -\\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right)-\\frac{1}{4}\\log_2\\left(\\frac{1}{4}\\right)-\\frac{1}{8}\\log_2\\left(\\frac{1}{8}\\right)-\\frac{1}{16}\\log_2\\left(\\frac{1}{16}\\right)-\\frac{1}{4}\\log_2\\left(\\frac{1}{64}\\right) = 2 \\ \\text{bits}$$"
      ],
      "metadata": {
        "id": "X2F0rnE7D90M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> > We see that the nonuniform distribution has a smaller entropy than the uniform one, because, we can take advantage of the nonuniform distribution by\n",
        "using shorter codes for the more probable events, at the expense of longer codes for the less probable events, in the hope of getting a shorter average code length."
      ],
      "metadata": {
        "id": "Bo9hx_hJE4c-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This relation between entropy and shortest coding length is a general one. The\n",
        "**noiseless coding theorem** (Shannon, 1948) states that the entropy is a lower bound on the number of bits needed to transmit the state of a random variable."
      ],
      "metadata": {
        "id": "JK8lUZIaFHRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mutual Information**"
      ],
      "metadata": {
        "id": "5xw3oX-JH30d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the mutual information between two classifications (note that this measure also applies to clusterings), i.e. the information that one classification has about the other."
      ],
      "metadata": {
        "id": "a13kyYPSH90b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Denote by $P(l)$, $l=1,\\ldots,M$ and $P'(l'), \\ l'=1,\\ldots, M$ the random variables associated with the classification $y$ and $y'$ (set of sets representation). That is, $P(l)$ is the probability of a pattern belonging to class $C_l$ in classification $y$.\n",
        "\n",
        "Let $P(l, l')$ represent the probability that a point belongs to $C_l$ in classification $y$ and to $C_{l'}$ in $y_l$, namely the joint distribution of random variables associated with the two classifications:\n",
        "$$P(l, l') = \\frac{|y_l \\cap y_{l'}|}{N}$$"
      ],
      "metadata": {
        "id": "3gH5ntYIIHeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define $I(y, y')$ the **mutual information** between the classification $y$, $y'$ to be equal to the mutual information between the associated random variables\n",
        "$$I(y, y') = \\sum_{l=1}^{M} \\sum_{l'=1}^{M} P(l,l') \\log \\frac{P(l,l')}{P(l) P(l')}$$"
      ],
      "metadata": {
        "id": "BzZj-1XwJT3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Intuitively, we can think of $I(y, y')$ in the following way: we are given a random point. The uncertainty about its class in $y'$ is measured by $H(y')$. Suppose now that we are told which class the point belongs to in $y$. How much does this knowledge reduce the uncertainty about $y'$? This reduction in uncertainty, averaged over all points, is equal to $I(y,y')$\n"
      ],
      "metadata": {
        "id": "TgwOz1XQJ_pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be shown that the **mutual information** is related to the **conditional entropy** through\n",
        "$$I(y, x) = H(x) - H(x|y) = H(y) - H(y|x)$$\n",
        "\n",
        "where the **conditional entropy** is defined as\n",
        "\n",
        "$$H(y|x) = -\\sum_{x} \\sum_y p(y,x) \\ln p(y| x)$$"
      ],
      "metadata": {
        "id": "SyupS42AKZVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mutual information between two random variables is always non-negative and symmetric:\n",
        "$$I(y, y') = I(y',y) \\geq 0$$\n",
        "\n",
        "- Mutual information can never exceed the total uncertainty in a classification\n",
        "$$ I(y,y') \\leq \\min\\left\\{ H(y), H(y')\\right\\}$$\n",
        "\n",
        "- When the two classifications are equal, and only then, we have\n",
        "$$ I(y,y') = H(y)= H(y')$$"
      ],
      "metadata": {
        "id": "GLfRmBKvLQjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Appendix**"
      ],
      "metadata": {
        "id": "EyQY2DZ7L31B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In 2007, Marina Meilá proposed a new comparison criterion for two classifications (It was actually proposed for clustering, but it works just as well in classification.) $y$, $y'$, based on mutual information.\n",
        "\n",
        "The quantity was called **Variation of Information**, which measures the amount of information lost and gained in changing from classification $y$ to classification $y'$. The VI is defined as:\n",
        "$$VI(y, y') = H(y) + H(y') -2I(y, y')$$\n",
        "\n",
        "This is the sum of two positive terms\n",
        "$$VI(y, y') = [H(y) -I(y,y')] + [H(y)-I(y,y')] = H(y|y') + H(y'|y)$$\n",
        "The two terms represent the conditional entropies $H(y|y')$ and $H(y'|y)$. The first term measures  the amount of information about $y$ that we lose, while the second measures the amount of information $y'$ that we have to gain, when going from classification $y$ to classification $y'$."
      ],
      "metadata": {
        "id": "CGTeu2VUMFrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important property of VI is that it is a **metric** (or **distance**) on the space of all classification."
      ],
      "metadata": {
        "id": "PWg9GSaMX19W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**"
      ],
      "metadata": {
        "id": "zGg_q9nb7eTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bishop, C. (2006). *Pattern Recognition and Machine Learning**. Springer.\n",
        "\n",
        "- Dougherty, G. (2013) *Pattern Recognition and Classification. An Introduction*, Springer.\n",
        "\n",
        "- Meilá, M. (2007) Comparing clusterings - an information based distance, in *Journal of Multivariate Analysis*, 98: 873-895.\n",
        "\n",
        "- Theodoridis, S. \\& Koutroumbas, K. (2009) *Pattern Recognition*, 4th ed., Academic Press."
      ],
      "metadata": {
        "id": "Mow11StQ7ey1"
      }
    }
  ]
}